{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing library which performs Text Statistics\n",
    "import textstat\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path of the text file \n",
    "text_path = \"E:\\\\test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the text file\n",
    "with open(text_path,encoding=\"utf-8\",errors='ignore') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We all have different reading goals that are based on a multitude of factors. Sometimes we fancy reading something \"light.\" When we read for entertainment or to gather information so we can stay current with newsworthy events, it does not necessarily require close, focused reading. One exposure is likely enough in these instances. Other times, however, we delve into heady text to research a medical condition, augment our understanding of a somewhat unfamiliar topic on a deeper level, or read an op-ed piece from an erudite and well-informed writer. If we were to limit our inventory of reading solely to the latter, more challenging type of text every time we read, it might be a laborious and cumbersomethough enlighteningprocess.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the syllable count\n",
    "syllable=textstat.syllable_count(text, lang='en_US')\n",
    "syllable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the sentence count\n",
    "sentence_count=textstat.sentence_count(text)\n",
    "sentence_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Readability Score using Flesh_Kincaid Grade level\n",
    "Flesh_Kincaid_Score=textstat.flesch_kincaid_grade(text)\n",
    "Flesh_Kincaid_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.34"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gunning Fog Readability Score\n",
    "Gunning_Fog_Score=textstat.gunning_fog(text)\n",
    "Gunning_Fog_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.7"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SMOG Index Readability Score\n",
    "SMOG_Score=textstat.smog_index(text)\n",
    "SMOG_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.82"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coleman-Liau Index Score\n",
    "Coleman_Score=textstat.coleman_liau_index(text)\n",
    "Coleman_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.9"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automated Readability Index\n",
    "Auto_Index_Score=textstat.automated_readability_index(text)\n",
    "Auto_Index_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.552000000000001"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the Average Grade Level\n",
    "Average_Grade=(Flesh_Kincaid_Score+Gunning_Fog_Score+SMOG_Score+Coleman_Score+Auto_Index_Score)/5\n",
    "Average_Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.82"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the Median of Grade Level\n",
    "Score=[Flesh_Kincaid_Score,Gunning_Fog_Score,SMOG_Score,Coleman_Score,Auto_Index_Score]\n",
    "Score.sort()\n",
    "Median_Grade=statistics.median(Score)\n",
    "Median_Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
